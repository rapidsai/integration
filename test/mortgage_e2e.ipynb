{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage Workflow\n",
    "\n",
    "## The Dataset\n",
    "The dataset used with this workflow is derived from [Fannie Maeâ€™s Single-Family Loan Performance Data](http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html) with all rights reserved by Fannie Mae. This processed dataset is redistributed with permission and consent from Fannie Mae.\n",
    "\n",
    "To acquire this dataset, please visit [RAPIDS Datasets Homepage](https://docs.rapids.ai/datasets/mortgage-data).  We've also added some cells below to make it easier to download the data, set parameters, and run the notebook.\n",
    "\n",
    "## Introduction\n",
    "The Mortgage workflow is composed of three core phases:\n",
    "\n",
    "1. ETL - Extract, Transform, Load\n",
    "2. Data Conversion\n",
    "3. ML - Training\n",
    "\n",
    "### ETL\n",
    "Data is \n",
    "1. Read in from storage\n",
    "2. Transformed to emphasize key features\n",
    "3. Loaded into volatile memory for conversion\n",
    "\n",
    "### Data Conversion\n",
    "Features are\n",
    "1. Broken into (labels, data) pairs\n",
    "2. Distributed across many workers\n",
    "3. Converted into compressed sparse row (CSR) matrix format for XGBoost\n",
    "\n",
    "### Machine Learning\n",
    "The CSR data is fed into a distributed training session with Dask-XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NCCL_P2P_DISABLE=1 # Necessary for NCCL < 2.4\n"
     ]
    }
   ],
   "source": [
    "%env NCCL_P2P_DISABLE=1 # Necessary for NCCL < 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_xgboost as dxgb_gpu\n",
    "import dask\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.delayed import delayed\n",
    "from dask.distributed import Client, wait\n",
    "import cudf\n",
    "\n",
    "import pynvml\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from collections import OrderedDict\n",
    "import gc\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LocalCUDACluster('tcp://127.0.0.1:36235', workers=2, threads=2, memory=135.11 GB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:36235</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>135.11 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:36235' processes=2 threads=2, memory=135.11 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = LocalCUDACluster(n_workers=2, threads_per_worker=1)  # Please change your n_workers amount to the number of GPUs you have\n",
    "print(cluster)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've made it easy for you download them and start your ETL.  Please select the range of years that you want to analyze.  The more years you pick, the longer it will take to download the data.  **If you already have the data, please edit and run only the first cell**\n",
    "\n",
    "The [RAPIDS Datasets Homepage](https://docs.rapids.ai/datasets/mortgage-data) will give you further information about the datasets, such as size.  Our largest dataset is 196GB uncompressed, so download the dataset that can fit in your own storage requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/taurean/h2o/rapids-workflows-script-mortgage_scale_benchmark/mortgage-e2e/data/mortgage/\n"
     ]
    }
   ],
   "source": [
    "# Edit and uncomment `end_year` if you already have the dataset you want downloaded.  Please keep track of that end year.  \n",
    "# If you don't have the data, you can use the below cells to download the size you want\n",
    "path = os.getcwd()\n",
    "data_dir = path + \"/data/mortgage/\" #your folder wher the mortgage data is located.\n",
    "start_year = 2000\n",
    "print(data_dir)\n",
    "#end_year = 2000 #uncomment only if you have the data downloaded to your machine already and know your end year.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_year = 2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have the dataset, please choose a span of years to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      " 78  448M   78  354M    0     0   552k      0  0:13:52  0:10:56  0:02:56  491k7  547k  0:13:32  0:08:20  0:05:12  528k      0  0:13:38  0:08:53  0:04:45  493k      0  0:13:47  0:10:09  0:03:38  503k"
     ]
    }
   ],
   "source": [
    "#One year\n",
    "!curl http://rapidsai-data.s3-website.us-east-2.amazonaws.com/notebook-mortgage-data/mortgage_2000.tgz --create-dirs -o data/mortgage/mortgage_compressed.tgz\n",
    "end_year = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two years\n",
    "!curl http://rapidsai-data.s3-website.us-east-2.amazonaws.com/notebook-mortgage-data/mortgage_2000-2001.tgz --create-dirs -o data/mortgage/mortgage_compressed.tgz\n",
    "end_year = 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Four years\n",
    "!curl http://rapidsai-data.s3-website.us-east-2.amazonaws.com/notebook-mortgage-data/mortgage_2000-2003.tgz --create-dirs -o data/mortgage/mortgage_compressed.tgz\n",
    "end_year = 2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eight years\n",
    "!curl http://rapidsai-data.s3-website.us-east-2.amazonaws.com/notebook-mortgage-data/mortgage_2000-2007.tgz --create-dirs -o data/mortgage/mortgage_compressed.tgz\n",
    "end_year = 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sixteen years\n",
    "!curl http://rapidsai-data.s3-website.us-east-2.amazonaws.com/notebook-mortgage-data/mortgage_2000-2015.tgz --create-dirs -o data/mortgage/mortgage_compressed.tgz\n",
    "end_year = 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seventeen years\n",
    "!curl http://rapidsai-data.s3-website.us-east-2.amazonaws.com/notebook-mortgage-data/mortgage_2000-2016.tgz --create-dirs -o data/mortgage/mortgage_compressed.tgz\n",
    "end_year = 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#untar your data\n",
    "!tar -xvzf data/mortgage/mortgage_compressed.tgz -C data/mortgage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the paths to data and set the size of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below serves as a quick start on mortgage data for the year 2000. Visit the [RAPIDS Datasets Homepage](https://docs.rapids.ai/datasets/mortgage-data) and update `data_url` below if you want to try other years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_data_path = data_dir + \"acq\"\n",
    "perf_data_path = data_dir + \"perf\"\n",
    "col_names_path = data_dir + \"names.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taurean/miniconda3/envs/rapids013-323-test/lib/python3.6/site-packages/fsspec/implementations/local.py:33: FutureWarning: The default value of auto_mkdir=True has been deprecated and will be changed to auto_mkdir=False by default in a future release.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   seller_name\"|\"new\n",
      "0                        WITMER FUNDING|LLC\"|\"Witmer\n",
      "1  WELLS FARGO CREDIT RISK TRANSFER SECURITIES TR...\n",
      "2                 WELLS FARGO BANK| NA\"|\"Wells Fargo\n",
      "3                WELLS FARGO BANK|N.A.\"|\"Wells Fargo\n",
      "4                  WELLS FARGO BANK|NA\"|\"Wells Fargo\n"
     ]
    }
   ],
   "source": [
    "#test to see you are reading from the proper directory.  You will see an output if you are.\n",
    "temp = cudf.read_csv(col_names_path)\n",
    "print(temp.head())\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_rmm_pool():\n",
    "    import rmm\n",
    "\n",
    "    rmm.reinitialize(pool_allocator=True)\n",
    "\n",
    "def initialize_rmm_no_pool():\n",
    "    import rmm\n",
    "\n",
    "    rmm.reinitialize(pool_allocator=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:33433': None, 'tcp://127.0.0.1:40907': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.run(initialize_rmm_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions to encapsulate the workflow into a single call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can edit your number of dask_cudf partitions here, depending on how many GPUs you have and how big your dataset is.  The number chosen can boost or reduce performance\n",
    "n_partitions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dask_task(func, **kwargs):\n",
    "    task = func(**kwargs)\n",
    "    return task\n",
    "\n",
    "def process_quarter_gpu(year=2000, quarter=1, perf_file=\"\"):\n",
    "    ml_arrays = run_gpu_workflow(quarter=quarter,year=year, perf_file=perf_file)\n",
    "    return ml_arrays\n",
    "\n",
    "def null_workaround(df, **kwargs):\n",
    "    for column, data_type in df.dtypes.items():\n",
    "        if str(data_type) == \"str\":\n",
    "            df[column] = df[column].astype('int32').fillna(-1)\n",
    "        if str(data_type) == \"category\":\n",
    "            df[column] = df[column].astype('int32').fillna(-1)\n",
    "        if str(data_type) in ['int8', 'int16', 'int32', 'int64', 'float32', 'float64']:\n",
    "            df[column] = df[column].fillna(np.dtype(data_type).type(-1))\n",
    "    return df\n",
    "\n",
    "def gpu_load_performance_csv(performance_path, **kwargs):\n",
    "    \"\"\" Loads performance data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dask GPU DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        \"loan_id\", \"monthly_reporting_period\", \"servicer\", \"interest_rate\", \"current_actual_upb\",\n",
    "        \"loan_age\", \"remaining_months_to_legal_maturity\", \"adj_remaining_months_to_maturity\",\n",
    "        \"maturity_date\", \"msa\", \"current_loan_delinquency_status\", \"mod_flag\", \"zero_balance_code\",\n",
    "        \"zero_balance_effective_date\", \"last_paid_installment_date\", \"foreclosed_after\",\n",
    "        \"disposition_date\", \"foreclosure_costs\", \"prop_preservation_and_repair_costs\",\n",
    "        \"asset_recovery_costs\", \"misc_holding_expenses\", \"holding_taxes\", \"net_sale_proceeds\",\n",
    "        \"credit_enhancement_proceeds\", \"repurchase_make_whole_proceeds\", \"other_foreclosure_proceeds\",\n",
    "        \"non_interest_bearing_upb\", \"principal_forgiveness_upb\", \"repurchase_make_whole_proceeds_flag\",\n",
    "        \"foreclosure_principal_write_off_amount\", \"servicing_activity_indicator\"\n",
    "    ]\n",
    "    \n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"monthly_reporting_period\", \"date\"),\n",
    "        (\"servicer\", \"str\"),\n",
    "        (\"interest_rate\", \"float64\"),\n",
    "        (\"current_actual_upb\", \"float64\"),\n",
    "        (\"loan_age\", \"float64\"),\n",
    "        (\"remaining_months_to_legal_maturity\", \"float64\"),\n",
    "        (\"adj_remaining_months_to_maturity\", \"float64\"),\n",
    "        (\"maturity_date\", \"date\"),\n",
    "        (\"msa\", \"float64\"),\n",
    "        (\"current_loan_delinquency_status\", \"int32\"),\n",
    "        (\"mod_flag\", \"category\"),\n",
    "        (\"zero_balance_code\", \"str\"),\n",
    "        (\"zero_balance_effective_date\", \"date\"),\n",
    "        (\"last_paid_installment_date\", \"date\"),\n",
    "        (\"foreclosed_after\", \"date\"),\n",
    "        (\"disposition_date\", \"date\"),\n",
    "        (\"foreclosure_costs\", \"float64\"),\n",
    "        (\"prop_preservation_and_repair_costs\", \"float64\"),\n",
    "        (\"asset_recovery_costs\", \"float64\"),\n",
    "        (\"misc_holding_expenses\", \"float64\"),\n",
    "        (\"holding_taxes\", \"float64\"),\n",
    "        (\"net_sale_proceeds\", \"float64\"),\n",
    "        (\"credit_enhancement_proceeds\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds\", \"float64\"),\n",
    "        (\"other_foreclosure_proceeds\", \"float64\"),\n",
    "        (\"non_interest_bearing_upb\", \"float64\"),\n",
    "        (\"principal_forgiveness_upb\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds_flag\", \"str\"),\n",
    "        (\"foreclosure_principal_write_off_amount\", \"float64\"),\n",
    "        (\"servicing_activity_indicator\", \"category\")\n",
    "    ])\n",
    "\n",
    "    print(performance_path)\n",
    "    pdf = dask_cudf.read_csv(performance_path, names=cols, delimiter='|', dtype=list(dtypes.values()), header= True, npartitions = n_partitions) \n",
    "    return pdf\n",
    "\n",
    "def gpu_load_acquisition_csv(acquisition_path, **kwargs):\n",
    "    \"\"\" Loads acquisition data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dask GPU DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        'loan_id', 'orig_channel', 'seller_name', 'orig_interest_rate', 'orig_upb', 'orig_loan_term', \n",
    "        'orig_date', 'first_pay_date', 'orig_ltv', 'orig_cltv', 'num_borrowers', 'dti', 'borrower_credit_score', \n",
    "        'first_home_buyer', 'loan_purpose', 'property_type', 'num_units', 'occupancy_status', 'property_state',\n",
    "        'zip', 'mortgage_insurance_percent', 'product_type', 'coborrow_credit_score', 'mortgage_insurance_type', \n",
    "        'relocation_mortgage_indicator'\n",
    "    ]\n",
    "    \n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"orig_channel\", \"str\"),\n",
    "        (\"seller_name\", \"str\"),\n",
    "        (\"orig_interest_rate\", \"float64\"),\n",
    "        (\"orig_upb\", \"int64\"),\n",
    "        (\"orig_loan_term\", \"int64\"),\n",
    "        (\"orig_date\", \"date\"),\n",
    "        (\"first_pay_date\", \"date\"),\n",
    "        (\"orig_ltv\", \"float64\"),\n",
    "        (\"orig_cltv\", \"float64\"),\n",
    "        (\"num_borrowers\", \"float64\"),\n",
    "        (\"dti\", \"float64\"),\n",
    "        (\"borrower_credit_score\", \"float64\"),\n",
    "        (\"first_home_buyer\", \"str\"),\n",
    "        (\"loan_purpose\", \"str\"),\n",
    "        (\"property_type\", \"str\"),\n",
    "        (\"num_units\", \"int64\"),\n",
    "        (\"occupancy_status\", \"str\"),\n",
    "        (\"property_state\", \"str\"),\n",
    "        (\"zip\", \"int64\"),\n",
    "        (\"mortgage_insurance_percent\", \"float64\"),\n",
    "        (\"product_type\", \"category\"),\n",
    "        (\"coborrow_credit_score\", \"float64\"),\n",
    "        (\"mortgage_insurance_type\", \"float64\"),\n",
    "        (\"relocation_mortgage_indicator\", \"str\")\n",
    "    ])\n",
    "    \n",
    "    print(acquisition_path)\n",
    "    adf = dask_cudf.read_csv(acquisition_path, names=cols, delimiter='|', dtype=list(dtypes.values()), header= True, npartitions = n_partitions)\n",
    "    return adf\n",
    "\n",
    "def gpu_load_names(**kwargs):\n",
    "    \"\"\" Loads names used for renaming the banks\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dask GPU DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    cols = [\n",
    "        'seller_name', 'new'\n",
    "    ]\n",
    "    \n",
    "    dtypes = OrderedDict([\n",
    "        (\"seller_name\", \"str\"),\n",
    "        (\"new\", \"category\"),\n",
    "    ])\n",
    "    ndf = dask_cudf.read_csv(col_names_path, names=cols, delimiter='|', dtype=list(dtypes.values()), header= True, npartitions = n_partitions)\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ever_features(gdf, **kwargs):\n",
    "    everdf = gdf[['loan_id', 'current_loan_delinquency_status']]\n",
    "    everdf = everdf.groupby('loan_id').max()\n",
    "    everdf = everdf.reset_index()\n",
    "    del(gdf)\n",
    "    everdf['ever_30'] = (everdf['current_loan_delinquency_status'] >= 1).astype('int8')\n",
    "    everdf['ever_90'] = (everdf['current_loan_delinquency_status'] >= 3).astype('int8')\n",
    "    everdf['ever_180'] = (everdf['current_loan_delinquency_status'] >= 6).astype('int8')\n",
    "    everdf = everdf.drop('current_loan_delinquency_status', axis=1)\n",
    "    return everdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_delinq_features(gdf, **kwargs):\n",
    "    \"\"\" Gathers loans with 30, 90 and 180 day delinquency status and merges them into one dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dask GPU DataFrame\n",
    "    \"\"\"\n",
    "    delinq_gdf = gdf[['loan_id', 'monthly_reporting_period', 'current_loan_delinquency_status']]\n",
    "    del(gdf)\n",
    "    delinq_30 = delinq_gdf.query('current_loan_delinquency_status >= 1')[['loan_id', 'monthly_reporting_period']].groupby('loan_id').min()\n",
    "    delinq_30 = delinq_30.reset_index()\n",
    "    delinq_30['delinquency_30'] = delinq_30['monthly_reporting_period']\n",
    "    delinq_30 = delinq_30.drop('monthly_reporting_period', axis=1)\n",
    "    delinq_90 = delinq_gdf.query('current_loan_delinquency_status >= 3')[['loan_id', 'monthly_reporting_period']].groupby('loan_id').min()\n",
    "    delinq_90 = delinq_90.reset_index()\n",
    "    delinq_90['delinquency_90'] = delinq_90['monthly_reporting_period']\n",
    "    delinq_90 = delinq_90.drop('monthly_reporting_period', axis=1)\n",
    "    delinq_180 = delinq_gdf.query('current_loan_delinquency_status >= 6')[['loan_id', 'monthly_reporting_period']].groupby('loan_id').min()\n",
    "    delinq_180 = delinq_180.reset_index()\n",
    "    delinq_180['delinquency_180'] = delinq_180['monthly_reporting_period']\n",
    "    delinq_180 = delinq_180.drop('monthly_reporting_period', axis=1)\n",
    "    del(delinq_gdf)\n",
    "    delinq_merge = delinq_30.merge(delinq_90, on=['loan_id'])\n",
    "    delinq_merge['delinquency_90'] = delinq_merge['delinquency_90'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    delinq_merge = delinq_merge.merge(delinq_180, on=['loan_id'])\n",
    "    delinq_merge['delinquency_180'] = delinq_merge['delinquency_180'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    del(delinq_30)\n",
    "    del(delinq_90)\n",
    "    del(delinq_180)\n",
    "    return delinq_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_ever_delinq_features(everdf_tmp, delinq_merge, **kwargs):\n",
    "    everdf = everdf_tmp.merge(delinq_merge, on=['loan_id'])\n",
    "    del(everdf_tmp)\n",
    "    del(delinq_merge)\n",
    "    everdf['delinquency_30'] = everdf['delinquency_30'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    everdf['delinquency_90'] = everdf['delinquency_90'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    everdf['delinquency_180'] = everdf['delinquency_180'].fillna(np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    return everdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_joined_df(gdf, everdf, **kwargs):\n",
    "    test = gdf[['loan_id', 'monthly_reporting_period', 'current_loan_delinquency_status', 'current_actual_upb']]\n",
    "    del(gdf)\n",
    "    test['timestamp'] = test['monthly_reporting_period']\n",
    "    test= test.drop('monthly_reporting_period', axis=1)\n",
    "    test['timestamp_month'] = test['timestamp'].dt.month\n",
    "    test['timestamp_year'] = test['timestamp'].dt.year\n",
    "    test['delinquency_12'] = test['current_loan_delinquency_status']\n",
    "    test = test.drop('current_loan_delinquency_status', axis=1)\n",
    "    test['upb_12'] = test['current_actual_upb']\n",
    "    test = test.drop('current_actual_upb', axis=1)\n",
    "    test['upb_12'] = test['upb_12'].fillna(999999999)\n",
    "    test['delinquency_12'] = test['delinquency_12'].fillna(-1)\n",
    "    \n",
    "    joined_df = test.merge(everdf, on=['loan_id'])\n",
    "    del(everdf)\n",
    "    del(test)\n",
    "    \n",
    "    joined_df['ever_30'] = joined_df['ever_30'].fillna(-1)\n",
    "    joined_df['ever_90'] = joined_df['ever_90'].fillna(-1)\n",
    "    joined_df['ever_180'] = joined_df['ever_180'].fillna(-1)\n",
    "    joined_df['delinquency_30'] = joined_df['delinquency_30'].fillna(-1)\n",
    "    joined_df['delinquency_90'] = joined_df['delinquency_90'].fillna(-1)\n",
    "    joined_df['delinquency_180'] = joined_df['delinquency_180'].fillna(-1)\n",
    "    \n",
    "    joined_df['timestamp_year'] = joined_df['timestamp_year'].astype('int32')\n",
    "    joined_df['timestamp_month'] = joined_df['timestamp_month'].astype('int32')\n",
    "    \n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_12_mon_features(joined_df, **kwargs):\n",
    "    testdfs = []\n",
    "    n_months = 12\n",
    "    for y in range(1, n_months + 1):\n",
    "        tmpdf = joined_df[['loan_id', 'timestamp_year', 'timestamp_month', 'delinquency_12', 'upb_12']]\n",
    "        tmpdf['josh_months'] = tmpdf['timestamp_year'] * 12 + tmpdf['timestamp_month']\n",
    "        tmpdf['josh_mody_n'] = ((tmpdf['josh_months'].astype('float64') - 24000 - y) / 12).astype('int64')\n",
    "        tmpdf = tmpdf.groupby(['loan_id', 'josh_mody_n']).agg({'delinquency_12': 'max','upb_12': 'min'})\n",
    "        tmpdf = tmpdf.reset_index()\n",
    "        tmpdf['delinquency_12'] = (tmpdf['delinquency_12']>3).astype('int32')\n",
    "        tmpdf['delinquency_12'] +=(tmpdf['upb_12']==0).astype('int32')\n",
    "        tmpdf['timestamp_year'] = (((tmpdf['josh_mody_n'] * n_months) + 24000 + (y - 1)) / 12).astype('int16')\n",
    "        tmpdf['timestamp_month'] = np.int8(y)\n",
    "        tmpdf = tmpdf.drop('josh_mody_n', axis=1)\n",
    "        testdfs.append(tmpdf)\n",
    "        del(tmpdf)\n",
    "    del(joined_df)\n",
    "\n",
    "    return dask_cudf.concat(testdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_joined_12_mon(joined_df, testdf, **kwargs):\n",
    "    joined_df = joined_df.drop('delinquency_12', axis=1)\n",
    "    joined_df = joined_df.drop('upb_12', axis=1)\n",
    "    joined_df['timestamp_year'] = joined_df['timestamp_year'].astype('int16')\n",
    "    joined_df['timestamp_month'] = joined_df['timestamp_month'].astype('int8')\n",
    "    return joined_df.merge(testdf, on=['loan_id', 'timestamp_year', 'timestamp_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_performance_delinquency(gdf, joined_df, **kwargs):\n",
    "    merged = null_workaround(gdf)\n",
    "    joined_df = null_workaround(joined_df)\n",
    "    joined_df['timestamp_month'] = joined_df['timestamp_month'].astype('int8')\n",
    "    joined_df['timestamp_year'] = joined_df['timestamp_year'].astype('int16')\n",
    "    merged['timestamp_month'] = merged['monthly_reporting_period'].dt.month\n",
    "    merged['timestamp_month'] = merged['timestamp_month'].astype('int8')\n",
    "    merged['timestamp_year'] = merged['monthly_reporting_period'].dt.year\n",
    "    merged['timestamp_year'] = merged['timestamp_year'].astype('int16')\n",
    "    merged = merged.merge(joined_df, on=['loan_id', 'timestamp_year', 'timestamp_month'])\n",
    "    merged = merged.drop('timestamp_year', axis=1)\n",
    "    merged = merged.drop('timestamp_month', axis=1)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_perf_acq_gdfs(perf, acq, **kwargs):\n",
    "    perf = null_workaround(perf)\n",
    "    acq = null_workaround(acq)\n",
    "    return perf.merge(acq, on=['loan_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_mile_cleaning(df, **kwargs):\n",
    "    drop_list = [\n",
    "        'loan_id', 'orig_date', 'first_pay_date', 'seller_name',\n",
    "        'monthly_reporting_period', 'last_paid_installment_date', 'maturity_date', 'ever_30', 'ever_90', 'ever_180',\n",
    "        'delinquency_30', 'delinquency_90', 'delinquency_180', 'upb_12',\n",
    "        'zero_balance_effective_date','foreclosed_after', 'disposition_date','timestamp'\n",
    "    ]\n",
    "    for column in drop_list:\n",
    "        df = df.drop(column, axis=1)\n",
    "    for col, dtype in df.dtypes.iteritems():\n",
    "        if str(dtype)=='category':\n",
    "            df[col] = df[col].cat.codes\n",
    "        df[col] = df[col].astype('float32')\n",
    "    df['delinquency_12'] = df['delinquency_12'] > 0\n",
    "    df['delinquency_12'] = df['delinquency_12'].fillna(False).astype('int32')\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].fillna(np.dtype(str(df[column].dtype)).type(-1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gpu_workflow(quarter=1, year=2000, perf_file=\"\", **kwargs):\n",
    "    \"\"\" Main function to perform ETL on the data.   \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dask GPU DataFrame\n",
    "    \"\"\"\n",
    "    names = gpu_load_names()\n",
    "    acq_gdf = gpu_load_acquisition_csv(acquisition_path= acq_data_path + \"/Acquisition_\"\n",
    "                                      + str(year) + \"Q\" + str(quarter) + \".txt\")\n",
    "    acq_gdf = acq_gdf.merge(names, on=['seller_name'], how=\"left\")\n",
    "    acq_gdf = acq_gdf.drop('seller_name', axis=1)\n",
    "    acq_gdf['seller_name'] = acq_gdf['new']\n",
    "    acq_gdf = acq_gdf.drop('new', axis=1)\n",
    "    perf_df_tmp = gpu_load_performance_csv(perf_file)\n",
    "    gdf = perf_df_tmp\n",
    "    everdf = create_ever_features(gdf)\n",
    "    delinq_merge = create_delinq_features(gdf)\n",
    "    everdf = join_ever_delinq_features(everdf, delinq_merge)\n",
    "    del(delinq_merge)\n",
    "    joined_df = create_joined_df(gdf, everdf)\n",
    "    testdf = create_12_mon_features(joined_df)\n",
    "    joined_df = combine_joined_12_mon(joined_df, testdf)\n",
    "    del(testdf)\n",
    "    perf_df = final_performance_delinquency(gdf, joined_df)\n",
    "    del(gdf, joined_df)\n",
    "    final_gdf = join_perf_acq_gdfs(perf_df, acq_gdf)\n",
    "    del(perf_df)\n",
    "    del(acq_gdf)\n",
    "    final_gdf = last_mile_cleaning(final_gdf)\n",
    "    return final_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform all of ETL with a single call to\n",
    "```python\n",
    "process_quarter_gpu(year=year, quarter=quarter, perf_file=file)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL for start_year:2000 and end_year:2007\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DoneAndNotDoneFutures(done=set(), not_done=set())"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# NOTE: The ETL calculates additional features which are then dropped before creating the XGBoost DMatrix.\n",
    "# This can be optimized to avoid calculating the dropped features.\n",
    "part_count = 4\n",
    "\n",
    "gpu_dfs = []\n",
    "gpu_time = 0\n",
    "quarter = 1\n",
    "year = start_year\n",
    "count = 0\n",
    "while year <= end_year:\n",
    "    for file in glob(os.path.join(perf_data_path + \"/Performance_\" + str(year) + \"Q\" + str(quarter) + \"*\")):\n",
    "        gpu_dfs.append(process_quarter_gpu(year=year, quarter=quarter, perf_file=file))\n",
    "        count += 1\n",
    "    quarter += 1\n",
    "    if quarter == 5:\n",
    "        year += 1\n",
    "        quarter = 1\n",
    "print(\"ETL for start_year:{} and end_year:{}\\n\".format(start_year,end_year))\n",
    "wait(gpu_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:33433': None, 'tcp://127.0.0.1:40907': None}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.run(initialize_rmm_no_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatonate all the dask dataframes into one.  We will `.persist()` and `wait` here as a milestone to reduce the persist/wait times later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddfs = dask_cudf.concat(gpu_dfs)\n",
    "_ = wait(ddfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose your operation.\n",
    "You can choose which operation you want to do: **Train on the whole dataset** for use in your OR **Train and test on the dataset** and see an RMSE score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create our data and labels from our dataframe.  We're going to use the `delinquency_12` column as our labels, and the rest of the dataframe as our data.  We have to wait for these to persist in order to use them for the DaskDMatrix, or you will throw an error.  Afterwards, we will train the data and create a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = ddfs[ddfs.columns.difference([\"delinquency_12\"])],ddfs[\"delinquency_12\"]\n",
    "X = X.persist()\n",
    "_ = wait(X)\n",
    "y = y.persist()\n",
    "_ = wait(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.dask.DaskDMatrix(client, \n",
    "        X,\n",
    "        y    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "trained_model = xgb.dask.train(client,\n",
    "                        {\n",
    "                         'learning_rate': 0.1,\n",
    "                          'max_depth': 8,\n",
    "                          'subsample': 1,\n",
    "                          'gamma': 0.1,\n",
    "                          'silent': True,\n",
    "                          'verbose_eval': True,\n",
    "                          'tree_method':'gpu_hist',\n",
    "                          'loss':              'ls',\n",
    "                          'objective':         'binary:logistic',\n",
    "                          'max_features':      'auto',\n",
    "                          'criterion':         'friedman_mse',\n",
    "                          'grow_policy':       'lossguide',\n",
    "                        },\n",
    "                        dtrain,\n",
    "                        num_boost_round=100, evals=[(dtrain, 'train')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a fully trained model.  You can continue doing any other analytics you'd like, but please don't run the next section or you can get memory allocation errors.  If you want to run it again, with a train and test dataset, you can below.  Hooray!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we do is split the data into a test and a training dataframe.  Let's persist/wait here again so that we don't have to wait as long when we do our test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = ddfs.random_split([0.80, 0.20]) # you can edit the split as you see fit.  Just make sure the two numbers equal 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create our data and labels for our test/train splits.  We're going to use the `delinquency_12` column as our labels, and the rest of the dataframe as our data.  We have to wait for these to persist in order to use them for the DaskDMatrix, or you will throw an error.  Afterwards, we will train a model using the train data and labels, then we will test the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = dask.persist(train_df[train_df.columns.difference([\"delinquency_12\"])], test_df[test_df.columns.difference([\"delinquency_12\"])])\n",
    "train_labels, test_labels = dask.persist(train_df[\"delinquency_12\"], test_df[\"delinquency_12\"])\n",
    "_ = wait(train_data)\n",
    "_ = wait(test_data)\n",
    "_ = wait(train_labels)\n",
    "_ = wait(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dtrain and dtest dask dmatrix\n",
    "dtrain = xgb.dask.DaskDMatrix(client, \n",
    "                              train_data,\n",
    "                              train_labels    )\n",
    "\n",
    "dtest = xgb.dask.DaskDMatrix(client, \n",
    "                             test_data,\n",
    "                             test_labels    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "\n",
    "trained_model = xgb.dask.train(client,\n",
    "                        {\n",
    "                         'learning_rate': 0.1,\n",
    "                          'max_depth': 8,\n",
    "                          'subsample': 1,\n",
    "                          'gamma': 0.1,\n",
    "                          'silent': True,\n",
    "                          'verbose_eval': True,\n",
    "                          'tree_method':'gpu_hist',\n",
    "                          'loss':              'ls',\n",
    "                          'objective':         'binary:logistic',\n",
    "                          'max_features':      'auto',\n",
    "                          'criterion':         'friedman_mse',\n",
    "                          'grow_policy':       'lossguide',\n",
    "                        },\n",
    "                        dtrain,\n",
    "                        num_boost_round=100, evals=[(dtrain, 'train')])\n",
    "\n",
    "#Predict the model\n",
    "prediction = xgb.dask.predict(client, trained_model['booster'], dtest)\n",
    "\n",
    "#form and test predictions form xgboost.dask output\n",
    "## get pred into an array\n",
    "pred = prediction.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will form our test_labels into an array that we can validate our predictions `pred` against it with an RMSE score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get test_labels into an array form\n",
    "ytest= test_labels.compute()\n",
    "ytest = ytest.to_array()\n",
    "ytest = ytest.astype(np.float32)\n",
    "\n",
    "##test prediction wih RMSE\n",
    "from cuml.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(ytest, pred))\n",
    "\n",
    "print(\"RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our notebook ends here.  You now have a fully trained model and tested it to get your RMSE.  You can continue doing any other analytics you'd like.  Hooray!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
